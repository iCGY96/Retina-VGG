import cv2
import numpy as np
from scipy.stats import pearsonr, entropy
from sklearn.metrics import roc_auc_score

def compute_cc(pred, gt):
    """
    Compute the Pearson Correlation Coefficient (CC) between the prediction and ground truth.
    Utilizes scipy.stats.pearsonr to calculate the correlation coefficient.
    """
    return pearsonr(pred.flatten(), gt.flatten())[0]

def compute_kl_div(pred, gt):
    """
    Compute the Kullback-Leibler (KL) divergence between the prediction and ground truth.
    Both maps are normalized to sum to 1 before computing the divergence.
    """
    eps = 1e-8
    pred_norm = pred / (np.sum(pred) + eps)
    gt_norm = gt / (np.sum(gt) + eps)
    # scipy.stats.entropy computes sum(pk * log(pk / qk))
    return entropy(gt_norm + eps, pred_norm + eps)

def compute_auc_judd(pred, fixation):
    """
    Compute the AUC Judd metric using sklearn's roc_auc_score.
    The fixation map should be a binary map indicating fixation locations.
    """
    pred_flat = pred.flatten()
    fixation_flat = fixation.flatten().astype(int)
    if np.sum(fixation_flat) == 0:
        return np.nan
    try:
        return roc_auc_score(fixation_flat, pred_flat)
    except ValueError:
        return np.nan

def evaluate_metrics(pred, gt):
    """
    Evaluate three saliency metrics: CC, KL divergence, and AUC Judd.
    The fixation map is generated by thresholding the ground truth at 50% of its maximum value.
    """
    cc = compute_cc(pred, gt)
    kl_div = compute_kl_div(pred, gt)
    fixmap = (gt >= 0.5 * np.max(gt)).astype(np.float32)
    auc_judd = compute_auc_judd(pred, fixmap)
    
    return {
        'CC': cc,
        'KLdiv': kl_div,
        'AUCJudd': auc_judd
    }
